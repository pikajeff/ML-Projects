{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjnlTEOn4rV"
      },
      "source": [
        "# Hand Gesture Recognition (Droids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diIaFfgwn4rb"
      },
      "source": [
        "### Header: Importing libraries and creating global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RCPWx_b-n4rc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Hold the background frame for background subtraction.\n",
        "background = None\n",
        "# Hold the hand's data so all its details are in one place.\n",
        "hand = None\n",
        "# Variables to count how many frames have passed and to set the size of the window.\n",
        "frames_elapsed = 0\n",
        "FRAME_HEIGHT = 200\n",
        "FRAME_WIDTH = 300\n",
        "\n",
        "CALIBRATION_TIME = 30\n",
        "BG_WEIGHT = 0.5\n",
        "OBJ_THRESHOLD = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc1SxDwsn4re"
      },
      "source": [
        "### HandData: A class to hold all the hand's detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H-3Kt-8Tn4rf"
      },
      "outputs": [],
      "source": [
        "class HandData:\n",
        "    top = (0,0)\n",
        "    bottom = (0,0)\n",
        "    left = (0,0)\n",
        "    right = (0,0)\n",
        "    centerX = 0\n",
        "    prevCenterX = 0\n",
        "    isInFrame = False\n",
        "    isWaving = False\n",
        "    fingers = None\n",
        "    gestureList = []\n",
        "\n",
        "    def __init__(self, top, bottom, left, right, centerX):\n",
        "        self.top = top\n",
        "        self.bottom = bottom\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.centerX = centerX\n",
        "        self.prevCenterX = 0\n",
        "        isInFrame = False\n",
        "        isWaving = False\n",
        "\n",
        "    def update(self, top, bottom, left, right):\n",
        "        self.top = top\n",
        "        self.bottom = bottom\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "    def check_for_waving(self, centerX):\n",
        "        self.prevCenterX = self.centerX\n",
        "        self.centerX = centerX\n",
        "\n",
        "        if abs(self.centerX - self.prevCenterX > 3):\n",
        "            self.isWaving = True\n",
        "        else:\n",
        "            self.isWaving = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOaLcv5n4rg"
      },
      "source": [
        "### write_on_image(): Write info related to the hand gesture and outline the region of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7jjIgNKGn4rh"
      },
      "outputs": [],
      "source": [
        "def write_on_image(frame):\n",
        "    text = \"Searching...\"\n",
        "\n",
        "    if frames_elapsed < CALIBRATION_TIME:\n",
        "        text = \"Calibrating...\"\n",
        "    elif hand == None or hand.isInFrame == False:\n",
        "        text = \"No hand detected\"\n",
        "    else:\n",
        "        if hand.isWaving:\n",
        "            text = \"Waving\"\n",
        "        elif hand.fingers == 0:\n",
        "            text = \"Rock\"\n",
        "        elif hand.fingers == 1:\n",
        "            text = \"Pointing\"\n",
        "        elif hand.fingers == 2:\n",
        "            text = \"Scissors\"\n",
        "\n",
        "    cv2.putText(frame, text, (10,20), cv2.FONT_HERSHEY_COMPLEX, 0.4,( 0 , 0 , 0 ),2,cv2.LINE_AA)\n",
        "    cv2.putText(frame, text, (10,20), cv2.FONT_HERSHEY_COMPLEX, 0.4,(255,255,255),1,cv2.LINE_AA)\n",
        "\n",
        "    # Highlight the region of interest.\n",
        "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255,255,255), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8wcmYoNn4ri"
      },
      "source": [
        "### get_region(): Separate the region of interest and preps it for edge detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tQwMQypmn4rj"
      },
      "outputs": [],
      "source": [
        "def get_region(frame):\n",
        "    # Separate the region of interest from the rest of the frame.\n",
        "    region = frame[region_top:region_bottom, region_left:region_right]\n",
        "    # Make it grayscale so we can detect the edges more easily.\n",
        "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
        "    # Use a Gaussian blur to prevent frame noise from being labeled as an edge.\n",
        "    region = cv2.GaussianBlur(region, (5,5), 0)\n",
        "\n",
        "    return region"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXHQPR6ln4rk"
      },
      "source": [
        "### get_average(): Create a weighted average of the background for image differencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GZlUxEScn4rk"
      },
      "outputs": [],
      "source": [
        "def get_average(region):\n",
        "    global background\n",
        "    # If w didnt captured the background yet make the current region the background\n",
        "    if background is None:\n",
        "        background = region.copy().astype(\"float\")\n",
        "        return\n",
        "    # Otherwise add this captured frame to the average of the backgrounds.\n",
        "    cv2.accumulateWeighted(region, background, BG_WEIGHT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeiiAJgvn4rl"
      },
      "source": [
        "### segment(): Use image differencing to separate the hand from the background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oGzj-lJen4rl"
      },
      "outputs": [],
      "source": [
        "def segment(region):\n",
        "    global hand\n",
        "    diff = cv2.absdiff(background.astype(np.uint8), region)\n",
        "\n",
        "    # Threshold that region with a strict 0 or 1 ruling so only the foreground remains.\n",
        "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "    # Get the contours of the region, which will return an outline of the hand.\n",
        "    (_, contours, _) = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # If we didn't get anything, there's no hand.\n",
        "    if len(contours) == 0:\n",
        "        if hand is not None:\n",
        "            hand.isInFrame = False\n",
        "        return\n",
        "    # Otherwise return a tuple of the filled hand (thresholded_region), along with the outline (segmented_region).\n",
        "    else:\n",
        "        if hand is not None:\n",
        "            hand.isInFrame = True\n",
        "        segmented_region = max(contours, key = cv2.contourArea)\n",
        "        return (thresholded_region, segmented_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6UZsPkAn4rl"
      },
      "source": [
        "### get_hand_data(): Find the extremities of the hand and put them in the global hand object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CcC1xzcZn4rm"
      },
      "outputs": [],
      "source": [
        "def get_hand_data(thresholded_image, segmented_image):\n",
        "    global hand\n",
        "\n",
        "    # Enclose the area around the extremities in a convex hull to connect all outcroppings.\n",
        "    convexHull = cv2.convexHull(segmented_image)\n",
        "\n",
        "    # Find the extremities for the convex hull and store them as points.\n",
        "    top    = tuple(convexHull[convexHull[:, :, 1].argmin()][0])\n",
        "    bottom = tuple(convexHull[convexHull[:, :, 1].argmax()][0])\n",
        "    left   = tuple(convexHull[convexHull[:, :, 0].argmin()][0])\n",
        "    right  = tuple(convexHull[convexHull[:, :, 0].argmax()][0])\n",
        "\n",
        "    # Get the center of the palm, so we can check for waving and find the fingers.\n",
        "    centerX = int((left[0] + right[0]) / 2)\n",
        "\n",
        "    # We put all the info into an object for detection\n",
        "    if hand == None:\n",
        "        hand = HandData(top, bottom, left, right, centerX)\n",
        "    else:\n",
        "        hand.update(top, bottom, left, right)\n",
        "\n",
        "    # Only check for waving every 6 frames.\n",
        "    if frames_elapsed % 6 == 0:\n",
        "        hand.check_for_waving(centerX)\n",
        "\n",
        "    # We count the number of fingers up every frame, but only change hand.fingers if\n",
        "    # 12 frames have passed\n",
        "    hand.gestureList.append(count_fingers(thresholded_image))\n",
        "    if frames_elapsed % 12 == 0:\n",
        "        hand.fingers = most_frequent(hand.gestureList)\n",
        "        hand.gestureList.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKuPU_MAn4rm"
      },
      "source": [
        "### count_fingers(): Count the number of fingers using a line intersecting fingertips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MsptdZvyn4rm"
      },
      "outputs": [],
      "source": [
        "def count_fingers(thresholded_image):\n",
        "\n",
        "    # Find the height at which we will draw the line to count fingers.\n",
        "    line_height = int(hand.top[1] + (0.2 * (hand.bottom[1] - hand.top[1])))\n",
        "\n",
        "    # Get the linear region of interest along where the fingers would be.\n",
        "    line = np.zeros(thresholded_image.shape[:2], dtype=int)\n",
        "\n",
        "    # Draw a line across this region of interest, where the fingers should be.\n",
        "    cv2.line(line, (thresholded_image.shape[1], line_height), (0, line_height), 255, 1)\n",
        "\n",
        "    # Do a bitwise AND to find where the line intersected the hand -- this is where the fingers are.\n",
        "    line = cv2.bitwise_and(thresholded_image, thresholded_image, mask = line.astype(np.uint8))\n",
        "\n",
        "    # Get the line's new contours. The contours are basically just little lines formed by gaps\n",
        "    (_, contours, _) = cv2.findContours(line.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "    fingers = 0\n",
        "\n",
        "    # This prevents a \"rock\" gesture from being mistaken for a finger.\n",
        "    for curr in contours:\n",
        "        width = len(curr)\n",
        "\n",
        "        if width < 3 * abs(hand.right[0] - hand.left[0]) / 4 and width > 5:\n",
        "            fingers += 1\n",
        "\n",
        "    return fingers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cX7Opv2n4rn"
      },
      "source": [
        "### most_frequent(): Returns the value in a list that appears most frequently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fF0kIrxBn4rn"
      },
      "outputs": [],
      "source": [
        "def most_frequent(input_list):\n",
        "    dict = {}\n",
        "    count = 0\n",
        "    most_freq = 0\n",
        "\n",
        "    for item in reversed(input_list):\n",
        "        dict[item] = dict.get(item, 0) + 1\n",
        "        if dict[item] >= count :\n",
        "            count, most_freq = dict[item], item\n",
        "\n",
        "    return most_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx2XR7Czn4rn"
      },
      "source": [
        "### Main function: Get input from camera and call functions to understand it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "-XqTSazIn4ro",
        "outputId": "8d68ee91-a9c2-4f45-9111-b6a1114af88c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c351a3cfda38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Store the frame from the video capture and resize it to the window size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFRAME_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAME_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Flip the frame over the vertical axis so that it works like a mirror, which is more intuitive to the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "# Our region of interest will be the top right part of the frame.\n",
        "region_top = 0\n",
        "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
        "region_left = int(FRAME_WIDTH / 2)\n",
        "region_right = FRAME_WIDTH\n",
        "\n",
        "frames_elapsed = 0\n",
        "\n",
        "capture = cv2.VideoCapture(1)\n",
        "\n",
        "while (True):\n",
        "    # Store the frame from the video capture and resize it to the window size.\n",
        "    ret, frame = capture.read()\n",
        "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
        "    # Flip the frame over the vertical axis so that it works like a mirror, which is more intuitive to the user.\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Separate the region of interest and prep it for edge detection.\n",
        "    region = get_region(frame)\n",
        "    if frames_elapsed < CALIBRATION_TIME:\n",
        "        get_average(region)\n",
        "    else:\n",
        "        region_pair = segment(region)\n",
        "        if region_pair is not None:\n",
        "            # If we have the regions segmented successfully, show them in another window for the user.\n",
        "            (thresholded_region, segmented_region) = region_pair\n",
        "            cv2.drawContours(region, [segmented_region], -1, (255, 255, 255))\n",
        "            cv2.imshow(\"Segmented Image\", region)\n",
        "\n",
        "            get_hand_data(thresholded_region, segmented_region)\n",
        "\n",
        "    # Write the action the hand is doing on the screen, and draw the region of interest.\n",
        "    write_on_image(frame)\n",
        "    # Show the previously captured frame.\n",
        "    cv2.imshow(\"Camera Input\", frame)\n",
        "    frames_elapsed += 1\n",
        "    # Check if user wants to exit.\n",
        "    if (cv2.waitKey(1) & 0xFF == ord('x')):\n",
        "        break\n",
        "\n",
        "capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}